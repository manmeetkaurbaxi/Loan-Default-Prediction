{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1-p5C5WLHHY"
   },
   "source": [
    "<center>\n",
    "\n",
    "# **Explainable AI Part 1: Data Preparation and Machine Learning**\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsdV7bb9SMz_"
   },
   "source": [
    "## **Introduction:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9JQOA6zbhv0"
   },
   "source": [
    "Explainable AI, also known as interpretable AI, refers to the ability of an artificial intelligence model to provide understandable explanations for its decisions or predictions. This transparency is particularly crucial for industries such as financial institutions and healthcare companies due to the following reasons:\n",
    "\n",
    "1- Regulatory Compliance: Both financial institutions and healthcare companies operate within heavily regulated environments. They are bound by laws and regulations that require transparency, fairness, and accountability in decision-making. Explainable AI helps meet these regulatory requirements by providing clear explanations for the factors that influenced a particular decision, thus ensuring compliance.\n",
    "\n",
    "2- Trust and Risk Assessment: Financial institutions and healthcare companies handle sensitive and critical information, including financial transactions, patient records, and medical diagnoses. By using explainable AI models, these organizations can enhance transparency and build trust with their customers, clients, and patients. When individuals understand the reasoning behind an AI-driven decision, they are more likely to trust and accept its outcomes.\n",
    "\n",
    "3- Bias Detection and Mitigation: AI models trained on historical data can unintentionally learn biases present in the data, which can lead to biased decisions. In finance and healthcare, biased decisions can have significant consequences. Explainable AI allows organizations to identify and address biases by providing insights into the model's decision-making process, making it easier to detect and mitigate any unfair or discriminatory practices.\n",
    "\n",
    "4- Model Validation and Auditing: Financial institutions and healthcare companies need to ensure that their AI models are accurate, reliable, and fair. Explainable AI enables model validation and auditing by allowing experts to understand how the model arrives at its predictions or decisions. It helps identify potential flaws or biases in the model's design or training data, enabling organizations to make necessary improvements.\n",
    "\n",
    "5- Error Diagnosis and Resolution: In complex domains such as finance and healthcare, AI models can make mistakes or provide incorrect predictions. Explainable AI facilitates error diagnosis by providing insights into the factors that influenced a particular decision. This information can aid experts in identifying the cause of errors and devising solutions to improve model performance and reliability.\n",
    "\n",
    "6- Human-AI Collaboration: Financial institutions and healthcare companies often involve human experts in decision-making processes. Explainable AI facilitates collaboration between humans and AI systems by providing interpretable explanations that humans can understand and validate. It allows experts to combine their domain knowledge with AI-generated insights, leading to more informed and effective decision-making.\n",
    "\n",
    "\n",
    "In our exciting project, we're diving deep into the heart of this challenge. We'll be using two powerful AI models, XGBoost and Random Forest, as our trusty guides. These models help us navigate the complex task of predicting loan defaults with accuracy and confidence.\n",
    "\n",
    "But the real magic lies in our XAI toolkit, consisting of SHAP (SHapley Additive exPlanations), Alibi, and counterfactuals. These cutting-edge techniques enable us to unravel the intricate threads of AI decision-making, bringing clarity and insight to the prediction process.\n",
    "\n",
    "For data scientists, our project offers a powerful arsenal of XAI techniques that enhance model interpretability. By employing SHAP, data scientists can gain a deep understanding of feature importance and how variables contribute to loan default predictions. This knowledge empowers them to refine and improve their models, resulting in more accurate and reliable predictions.\n",
    "\n",
    "Regulators and stakeholders benefit greatly from XAI in loan default prediction. The transparency and explainability provided by XAI techniques, such as Alibi, allow regulators to ensure compliance with legal and ethical standards. They can verify that lending decisions are based on fair and non-discriminatory factors, reducing the potential for bias and promoting equal opportunities for borrowers.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e78b_idcMDT"
   },
   "source": [
    "## **Data Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftOER2zobtzI"
   },
   "source": [
    "****\n",
    "Data\n",
    "****\n",
    "| Column Name          | Data Type                | Description                   |\n",
    "|----------------------|--------------------------|-------------------------------|\n",
    "| User id              | Integer                  | Unique user identifier        |\n",
    "| Loan category        | String                   | Categorical variable          |\n",
    "| Amount               | Integer                  | Loan amount                   |\n",
    "| Interest Rate        | Integer                  | Interest rate                 |\n",
    "| Tenure               | Integer                  | Loan tenure                   |\n",
    "| Employment type      | String                   | Categorical variable          |\n",
    "| Tier of Employment   | Categorical and Ordinal  | Employment tier classification|\n",
    "| Industry             | Categorical              | Industry type                 |\n",
    "| Role                 | Categorical              | Role description              |\n",
    "| Work Experience      | Categorical and Ordinal  | Work experience category      |\n",
    "| Total Income(PA)     | Integer                  | Total annual income           |\n",
    "| Gender               | Categorical              | Gender of the user            |\n",
    "| Married              | Categorical              | Marital status                |\n",
    "| Dependents           | Integer                  | Number of dependents          |\n",
    "| Home                 | Categorical              | Housing category              |\n",
    "| Pincode              | Unknown                  | Pincode information           |\n",
    "| Social Profile       | Categorical              | Social profile of the user    |\n",
    "| Is_verified          | Categorical              | Verification status           |\n",
    "| Delinq_2yrs          | Integer                  | Number of delinquencies       |\n",
    "| Total Payment        | Integer                  | Total payment received        |\n",
    "| Received Principal   | Integer                  | Principal amount received     |\n",
    "| Interest Received    | Integer                  | Interest amount received      |\n",
    "| Number of loans      | Integer                  | Number of loans taken          |\n",
    "| Defaulter            | Categorical              | Loan defaulter classification |\n",
    "\n",
    "****\n",
    "**Column to be predicted \"Defaulter\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oVMKx7dVtGx"
   },
   "source": [
    "## **Importing Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1687238008967,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "SF0DudF7Obz7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4958,
     "status": "ok",
     "timestamp": 1687238362605,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "kuQkX6NuhXrr"
   },
   "outputs": [],
   "source": [
    "from Source.setup import *\n",
    "from Source.eda import *\n",
    "from Source.machine_learning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQUx0VKAbDle"
   },
   "source": [
    "## **Reading and Merging Data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 163540,
     "status": "ok",
     "timestamp": 1687238526132,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "D2BSaoPa_LjA"
   },
   "outputs": [],
   "source": [
    "# Specify the file path of the Excel file containing the dataset\n",
    "path = '../Input/raw/Credit_Risk_Dataset.xlsx'\n",
    "\n",
    "# Call the function to read the Excel data\n",
    "sheet_names= ['loan_information','Employment','Personal_information','Other_information']\n",
    "\n",
    "loan_information, employment, personal_information, other_information = read_excel_data(path, sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1687238526134,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "ohfG31FqQlKE",
    "outputId": "5601dffb-14d4-4b9a-e06c-9379aae8be27"
   },
   "outputs": [],
   "source": [
    "employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 862,
     "status": "ok",
     "timestamp": 1687238526988,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "GtQ622FgeP2U",
    "outputId": "061e6f64-fb3d-476f-e78f-c780f021040a"
   },
   "outputs": [],
   "source": [
    "# Merge 'loan_information' and 'Employment' dataframes based on 'User_id'\n",
    "merged_df = pd.merge(loan_information, employment, left_on='User_id', right_on='User id')\n",
    "\n",
    "# Merge the previously merged dataframe with 'personal_information' based on 'User_id'\n",
    "merged_df = pd.merge(merged_df, personal_information, left_on='User_id', right_on='User id')\n",
    "\n",
    "# Merge the previously merged dataframe with 'other_information' based on 'User_id'\n",
    "merged_df = pd.merge(merged_df, other_information, left_on='User_id', right_on='User_id')\n",
    "\n",
    "df=merged_df\n",
    "# Display the first few rows of the merged dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvFvFQSK_pT2"
   },
   "source": [
    "## **Exploratory Data Analysis & Data Preparation**\n",
    "****\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O58U_bGhAmwj"
   },
   "source": [
    "**Goal: To Understand**\n",
    "\n",
    "*   **Identify data types of each column**\n",
    "*  **Understand basic stastistics of all numerical columns**\n",
    "*   **Check missing values**\n",
    "*   **Plan for handling missing values**\n",
    "****\n",
    "*   **Analyze the distribution of each column**\n",
    "*   **Assess the quality of data based on the distribution**\n",
    "*   **Identify any skewness in the data?**\n",
    "*   **Investigate the reasons behind data skewness**\n",
    "\n",
    "****\n",
    "*   **Identify and handle categorical features**\n",
    "*   **Identify and handle ordinal features**\n",
    "****\n",
    "\n",
    "*   **Examine the correlation between different numeric variables**\n",
    "*   **Examine the correlation between categorical variables**\n",
    "\n",
    "****\n",
    "*   **Fix data imbalance**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apQpTS0g26W8"
   },
   "source": [
    "### **Basic Statistics**\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1687238526990,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "Nk5Jc16K_t7O",
    "outputId": "64759afe-dd4b-4c4b-9741-c8275c108bfe"
   },
   "outputs": [],
   "source": [
    "# Display data types for each column in the DataFrame. Goal is to see if there is any column with the wrong data type.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1687238526993,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "S0GVGg4-rDxY",
    "outputId": "eb89c2cc-abb9-4af2-fb01-28cdd7dd756d"
   },
   "outputs": [],
   "source": [
    "# We can use describe pandas function to learn basic statistics of all numerical columns in our data.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMeB5tKTjplX"
   },
   "source": [
    "### **Handling Missing Values in Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1687238526994,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "jKSA-EY0rTH6",
    "outputId": "461ee11c-2ac2-427a-9308-c863102f76da"
   },
   "outputs": [],
   "source": [
    "# Let's check how many missing values do we have in each column in our dataframe\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip74kTlaYXA-"
   },
   "source": [
    "**Analysis: Handling Missing Values**\n",
    "* Social profile: Create a new category for NA values.\n",
    "* Is verified: Create a new category for NA values.\n",
    "* Married: Create a new category for NA values.\n",
    "* Industry: Consider dropping missing values.\n",
    "* Work experience: Consider dropping missing values.\n",
    "* Amount: Evaluate the impact of removing rows with missing values on the data distribution.\n",
    "* Employment type: Determine the appropriate approach for handling missing values.\n",
    "* Tier of employment: Determine the appropriate approach for handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1687238526997,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "ClyhURt6U8zD"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the 'Industry' and 'Work Experience' columns as the data in 'Industry' is meaningless due to encryption, and 'Work Experience' is inconsistent in the dataset, treating it as an object datatype variable which may impact model performance.\n",
    "df = df.dropna(subset=['Industry', 'Work Experience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1687238527449,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "I-QjzgpFXM60",
    "outputId": "a85d278e-9fed-47bc-c683-a52a674f88e5"
   },
   "outputs": [],
   "source": [
    "# Call the function to replace null values with \"missing\"\n",
    "replace_with='missing'\n",
    "columns_to_replace = ['Social Profile', 'Is_verified', 'Married', 'Employmet type']\n",
    "\n",
    "\n",
    "df= replace_null_values_with_a_value(df, columns_to_replace, replace_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1687238527450,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "Zu1Bfhk8atep",
    "outputId": "3ec264c8-af1d-4813-a6f8-888f3d8b2c51"
   },
   "outputs": [],
   "source": [
    "#Create a new variable \"amount_missing\" to indicate if the 'Amount' is missing or not. Assign 1 if 'Amount' is null, otherwise assign 0.\n",
    "df['amount_missing'] = np.where(df['Amount'].isnull(), 1, 0)\n",
    "\n",
    "#Replace the null values in the 'Amount' column with the value \"-1000\" to differentiate them from the rest of the data.\n",
    "replace_with= - 1000\n",
    "columns_to_replace = ['Amount']\n",
    "\n",
    "df= replace_null_values_with_a_value(df, columns_to_replace,replace_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1687238527451,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "FjZeSLUjckSH"
   },
   "outputs": [],
   "source": [
    "# Replace the null values in the 'Tier of Employment' column with the string \"Z\" to categorize them separately.\n",
    "replace_with='Z'\n",
    "columns_to_replace = ['Tier of Employment']\n",
    "\n",
    "df= replace_null_values_with_a_value(df, columns_to_replace,replace_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1687238527452,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "LfHOWTocWv7T",
    "outputId": "c283a034-86cc-4a4b-fb5f-5c1c924295aa"
   },
   "outputs": [],
   "source": [
    "#Check for null rows in the DataFrame to confirm if the data is clean and does not contain any missing values that could potentially impact the performance of the model.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTGP2ncAbpCh"
   },
   "source": [
    "### **Drop categorical columns with too many categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1687238527453,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "Emfs-pB2F4cU",
    "outputId": "8ba03454-c0e0-4567-9646-c6f6bec27814"
   },
   "outputs": [],
   "source": [
    "# Call the function to print the number of unique values in all columns\n",
    "unique_values_each_column(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnCokCRsEo2Y"
   },
   "source": [
    "**Observations**\n",
    "- Some columns are ordinal and those categorical variables would need to be treated differently during categorical encoding.\n",
    "- Address the challenge of categorical columns with a large number of categories. Determine which categories fall into this category and develop a strategy for handling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1687238528128,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "O_DjCmXLU5oh",
    "outputId": "edee60ce-4323-4bfb-d44c-317b866f7b9b"
   },
   "outputs": [],
   "source": [
    "# Dropping Industry Column and User_IDs as it doesn't give any significant information\n",
    "# Drop 'Pincode' column: Considering privacy concerns, the 'Pincode' data is encrypted. To address these concerns, it is prudent to remove the 'Pincode' column from the dataset.\n",
    "columns_to_drop = ['Industry', 'User_id','User id_x','User id_y','Pincode','Role']\n",
    "\n",
    "# Call the function to drop columns\n",
    "drop_columns(df, columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzpTa58zrkIi"
   },
   "source": [
    "**Analysis on Dropping Industry and Pincode**\n",
    "\n",
    "- Drop 'Industry': As a non-ordinal categorical variable, we need to address the issue of the high number of categories to prevent the model's dimensionality from becoming too large. Since there is no effective way to group these categories into broader categories, it is recommended to drop this column for the time being.\n",
    "\n",
    "- Convert 'Pincode' to latitude and longitude variables: Considering that 'Pincode' represents location data, it might be beneficial to transform it into latitude and longitude variables. This conversion can provide more meaningful spatial information that can potentially enhance the analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO32vXfTrmQx"
   },
   "source": [
    "**Analysis on Current DataFrame**\n",
    "- Employment Type and Tier of Employment: When the employment type is missing, the corresponding tier of employment is also empty. It is important to address these missing values and find an appropriate approach to handle them.\n",
    "\n",
    "- Missing Data in the Married Column: The presence of missing data in the 'Married' column raises questions about how to interpret this missing information. Considering the impact on machine learning models' performance, it is crucial to decide whether to treat it as a new category or explore the reasons behind the missing values.\n",
    "\n",
    "- Empty Social Profiles: Empty social profiles can be treated as a distinct category to account for the missing information.\n",
    "\n",
    "- Is Verified Column: The reason behind the empty values in the 'Is Verified' column is unclear. Assigning a new category to represent these missing values would be a suitable approach.\n",
    "\n",
    "- Removal of Industry and Work Experience: Since the number of rows with missing values in the 'Industry' and 'Work Experience' columns is minimal (only 4 rows), removing them is unlikely to have a significant impact. Hence, removing these rows can be considered.\n",
    "\n",
    "- Handling Empty Amounts: Decisions need to be made regarding the rows with missing values in the 'Amount' column. Since the missing values constitute a substantial portion (more than 20%) of the data, careful consideration is required to understand the implications of removing these rows on the data distribution and model performance.\n",
    "- Drop Industry Variable: The 'Industry' variable has 120,000 occurrences of the value 0, which suggests that it has been encoded and lacks meaningful interpretation. Therefore, dropping the 'Industry' variable temporarily and revisiting it later could be a suitable approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFvKV6FdFdcY"
   },
   "source": [
    "### **Multicollinearity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmtoALAEMuz_"
   },
   "source": [
    "To assess the relationships between variables in the input DataFrame, we compute the Spearman correlation matrix, that tells how much two variables are correlated.\n",
    "\n",
    "By examining the heatmap, we can identify potential concerns related to multicollinearity, which occurs when two or more independent variables exhibit high correlation. Multicollinearity can impact the interpretation of the model and lead to overfitting. In such instances, it may be necessary to remove one of the correlated variables to mitigate these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848
    },
    "executionInfo": {
     "elapsed": 1274,
     "status": "ok",
     "timestamp": 1687238529376,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "kzPMBvSNcYNl",
    "outputId": "b85010f5-3386-49da-bd70-16958d638cb9"
   },
   "outputs": [],
   "source": [
    "# Multicollinarity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression mode\n",
    "\n",
    "correlation_heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-gQXjCBnch4"
   },
   "source": [
    "**Observation from the heatmap**\n",
    "\n",
    "No two variables have high correlation with each other, so there is no issue of multicollinearity. It's safe to use all variables in machine learning model building.\n",
    "\n",
    "\n",
    "**Spearman Correlation Coefficient**\n",
    "\n",
    "Spearman works best if there are nonlinear relationships between different variables.\n",
    "\n",
    "****\n",
    "\n",
    "**We can confirm non-linear relationship b/w features by looking at pair-wise scatter plots below**\n",
    "\n",
    "There are non-linear relationships b/w features, so our decision of going with spearman was CORRECT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpxZb4yCNJDb"
   },
   "source": [
    "#### **Scatter Plots to visualize correlations between x variables**\n",
    "\n",
    "The following code generates a scatter plot matrix, also known as a pair plot, of all numeric features in the input DataFrame using the seaborn library.\n",
    "\n",
    "The diagonal of the plot matrix shows a histogram of each variable's distribution. This allows for visual inspection of the pairwise relationships between variables, which can be useful for identifying patterns, trends, correlations, or potential outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 116301,
     "status": "ok",
     "timestamp": 1687238645666,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "0geayW2JnkXJ",
    "outputId": "45e9d294-6dcd-4f60-b848-2dae47fdae36"
   },
   "outputs": [],
   "source": [
    "# Let's plot all interaction scatter plots using seaborn\n",
    "\n",
    "# Call the function to plot pairwise scatter plots\n",
    "plot_pairwise_scatter(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6I54X4Ho-k6"
   },
   "source": [
    "\n",
    "**Analysis**\n",
    "\n",
    "****\n",
    "\n",
    "- Scatterplots can help us confirm multicollinearity between two variables. We  can look at the scatterplot to check if there is any pattern or correlation between two variables.**\n",
    "\n",
    "- Multicollinearity makes explainability less trustworthy as change in one variablte will not only impact the target variable but also impact other X variables. Means how much does a variable impact target variable would be hard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFr5YzA13vmj"
   },
   "source": [
    "### **Skewness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sP2sYwYSX3CI"
   },
   "source": [
    "#### **Understand Skewness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 11966,
     "status": "ok",
     "timestamp": 1687238657609,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "uO52GMgDjJhW",
    "outputId": "fb9d6b66-8487-4ea2-9cb0-e88ace2aad9d"
   },
   "outputs": [],
   "source": [
    "# To identify anomalies in each column, it is essential to examine the distributions of the variables in the dataset.\n",
    "\n",
    "# Call the function to plot histograms\n",
    "Feature_Distributions_Histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqSZKYsIzakx"
   },
   "source": [
    "**Concept of skewness:**\n",
    "\n",
    "Imagine a seesaw in a playground. If the seesaw is perfectly balanced, it means both ends are at the same height and there is no tilt. Similarly, a symmetric distribution has equal amounts of data on both sides of the central point, resulting in a balance.\n",
    "\n",
    "However, if the seesaw is tilted to one side, it indicates an imbalance. Similarly, if a distribution is skewed, it means there is more data concentrated on one side compared to the other. This imbalance causes the distribution to have a \"tail\" that stretches towards the side with fewer data points.\n",
    "\n",
    "Skewness helps us understand the direction and degree of this imbalance. If the tail stretches to the right side, we call it positive skewness, indicating a longer right tail. Conversely, if the tail stretches to the left side, we call it negative skewness, indicating a longer left tail.\n",
    "\n",
    "**Positive skewness:**\n",
    "\n",
    "Imagine a line plot or histogram representing the heights of students in a class. In a class with right skewness, most students will have heights clustered towards the shorter side, and there will be a few students with taller heights. The right tail of the distribution will be longer, indicating the presence of outliers or extreme values on the taller side.\n",
    "\n",
    "**Negative Skewness:**\n",
    "Consider a line plot or histogram representing the time spent studying for an exam. In a situation with left skewness, most students might have relatively high study times, and there will be a few students with very low study times. The left tail of the distribution will be longer, indicating the presence of outliers or extreme values on the lower side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIFpj_luwJXk"
   },
   "source": [
    "**Analysis after Distribution:**\n",
    "\n",
    "1. Amount: The distribution of the 'Amount' variable is right-skewed, indicating that a majority of loan amounts are lower, while a few instances have higher values.\n",
    "\n",
    "2. Employment Type: The distribution of the 'Employment Type' variable shows an imbalance, suggesting that certain employment types may be overrepresented in the dataset compared to others.\n",
    "\n",
    "3. Work Experience: The 'Work Experience' variable also exhibits imbalanced data, implying that certain levels of work experience may be more prevalent than others.\n",
    "\n",
    "4. Pincode: The 'Pincode' variable contains a large number of categories, which may pose challenges for analysis. Considering converting it into latitude and longitude coordinates could offer a more manageable representation.\n",
    "\n",
    "5. Delinq_2years: The distribution of the 'Delinq_2years' variable is right-skewed, indicating that most individuals have a low number of delinquencies, while a few have a higher count.\n",
    "\n",
    "6. Payment: The 'Payment' variable displays a right-skewed distribution, suggesting that the majority of payment amounts are lower, with a few instances of higher payments.\n",
    "7. Received Principal: The distribution of the 'Received Principal' variable is right-skewed, indicating that most individuals have received a lower principal amount, while a few have received a higher amount.\n",
    "\n",
    "8. Interest Received: The 'Interest Received' variable exhibits a right-skewed distribution, suggesting that the majority of individuals have received a lower interest amount, while a few have received a higher interest payment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCAyDiSOMhI2"
   },
   "source": [
    "**Outcome**\n",
    "\n",
    "The resulting histograms will show the distribution of values in each column and can help identify potential outliers or anomalies in the data. For example, if a column has a very skewed distribution or contains a large number of extreme values, this may indicate that the data is not representative or that there are errors or issues with the data collection process.\n",
    "\n",
    "Note that generating histograms for a large number of columns can be computationally intensive and may take some time to run, especially for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NXZYBQ2v_C0"
   },
   "source": [
    "#### **Fixing Skewness in the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6PF52mdyRYP"
   },
   "source": [
    "**Skewness**\n",
    "\n",
    "- Skewness is a statistical measure that describes the asymmetry or lack of symmetry in a distribution. It provides insights into the shape of the distribution and the relative positioning of the mean, median, and mode.\n",
    "- A positive skewness value indicates a right-skewed distribution, where the tail is elongated towards the right. A negative skewness value indicates a left-skewed distribution, where the tail is elongated towards the left. A skewness value of 0 indicates a symmetric distribution.\n",
    "- By quantifying skewness, we can gain a numerical understanding of the distribution's asymmetry and further analyze its implications in data analysis and modeling\n",
    "\n",
    "\n",
    "**How to quantify skewness?**\n",
    "\n",
    "* skewness = 3 * (mean - median) / standard deviation.\n",
    "\n",
    "****\n",
    "**Question to consider**\n",
    "\n",
    "* Should we remove outliers/extreme values first and then fix skewness or the otherway around?\n",
    "****\n",
    "**Decision**\n",
    "* To address the presence of outliers in skewed data, we'll first correct the skewness.\n",
    "* One approach to mitigating the impact of outliers is by utilizing the z-score. The z-score measures how many standard deviations an individual data point is away from the mean. By setting a threshold, such as 3 standard deviations, data points that exceed this threshold can be considered as outliers and subsequently removed from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iO0or0g1zWRD"
   },
   "source": [
    "**Let's print skewness in each feature and use log transformation to fix skewness.**\n",
    "\n",
    "**Note**\n",
    "\n",
    "It is important to note that there are numerous features in the dataset with a value of 0. To address this issue and normalize the data, we can apply a log transformation specifically to the non-zero values. By taking the logarithm of these values, we can achieve a more symmetric distribution and reduce the impact of extreme values. This transformation can be particularly useful when working with skewed data or variables that exhibit a wide range of values.\n",
    "\n",
    "Let's only transform features if skewness is in the following range\n",
    "* **Skewness < -3 OR Skewness > 3**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1687238657610,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "o38LW3FP2rYq"
   },
   "outputs": [],
   "source": [
    "# Add all the features to check and fix skewness in features_log array\n",
    "features_log= ['Amount','Interest Rate','Tenure(years)','Dependents','Total Payement ','Received Principal','Interest Received']\n",
    "\n",
    "df= fix_skewness(df, features_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJmMYgUz88Sa"
   },
   "source": [
    "### **One Hot Encoding of Categorical Features and Ordinal Encoding of Ordinal Features**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miywf_EGRgH-"
   },
   "source": [
    "**Categorical and Ordinal Variables**\n",
    "\n",
    "Categorical variables refer to variables that represent discrete categories or labels, such as gender (male/female), marital status (single/married/divorced), or product types (A/B/C). These variables do not have a specific numerical order or hierarchy.\n",
    "\n",
    "On the other hand, ordinal variables also represent discrete categories, but they have an inherent order or ranking associated with them. Examples of ordinal variables include education level (elementary/middle/high school/college), employment status (unemployed/part-time/full-time), or customer satisfaction rating (low/medium/high)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tCprm7ET92m"
   },
   "source": [
    "#### **Categorical Hot-Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1687238657611,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "OIUEIRurVDNZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Add all categorical features for categorical one-hot encoding in categorical_features array\n",
    "data = df\n",
    "categorical_features= [\"Gender\", \"Married\", \"Home\", \"Social Profile\", \"Loan Category\", \"Employmet type\",\"Is_verified\", ]\n",
    "\n",
    "# Perform one-hot encoding using pandas get_dummies() function\n",
    "encoded_data = pd.get_dummies(data, columns=categorical_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg4k5llOXkYI"
   },
   "source": [
    "#### **Ordinal Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1687238657612,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "tQI71rbOXpxs"
   },
   "outputs": [],
   "source": [
    "# Define the ordinal categorical features array\n",
    "ordinal_features = [\"Tier of Employment\", \"Work Experience\"]\n",
    "\n",
    "# Define the pandas DataFrame for encoding\n",
    "data = encoded_data\n",
    "\n",
    "# Create a custom mapping of categories to numerical labels\n",
    "tier_employment_order= list(encoded_data[\"Tier of Employment\"].unique())\n",
    "tier_employment_order.sort()\n",
    "\n",
    "work_experience_order= [ 0, '<1', '1-2', '2-3', '3-5', '5-10','10+']\n",
    "\n",
    "custom_mapping = [tier_employment_order, work_experience_order]\n",
    "\n",
    "# Call the function to perform ordinal encoding\n",
    "data = perform_ordinal_encoding(data, ordinal_features, custom_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1687238657612,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "rMe7g9GWqJYs",
    "outputId": "0a588b9a-faae-4415-b3f8-ddeed575bf5d"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfWfLNyqnvyW"
   },
   "source": [
    "### **Fix data imbalance in the target variable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rm-mvARDpa7K"
   },
   "source": [
    "**Oversampling**\n",
    "\n",
    " Increase the number of instances in the minority class (defaulters) by duplicating existing examples or generating synthetic examples to achieve a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1687238657613,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "OMbx7PjjLVXX"
   },
   "outputs": [],
   "source": [
    "# Specify the name of the target variable column\n",
    "target_column=\"Defaulter\"\n",
    "\n",
    "X, y= fix_imbalance_using_oversamping(data, target_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnzGYZzYOR6N"
   },
   "source": [
    "**SMOTE**\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular technique used in machine learning and data mining to address the issue of imbalanced datasets. Imbalanced datasets are characterized by a significant difference in the number of instances between the classes, where one class (the minority class) has a much smaller representation than the other class(es) (the majority class(es)).\n",
    "\n",
    "SMOTE works by generating synthetic samples for the minority class to balance the class distribution. It does this by creating synthetic examples along the line segments connecting minority class instances. Here's a high-level overview of how SMOTE works:\n",
    "\n",
    "For each instance in the minority class, SMOTE selects one or more of its k nearest neighbors from the same class. The value of k is a user-defined parameter.\n",
    "Synthetic samples are created by randomly selecting one or more of the nearest neighbors and using them to form new samples. This is done by interpolating the feature values between the selected instance and its neighbor(s). For example, if there are two nearest neighbors, SMOTE can create a synthetic sample by taking a weighted average of the feature values of the two neighbors.\n",
    "The synthetic samples are added to the dataset, effectively increasing the representation of the minority class. This process is repeated until the desired balance between the classes is achieved.\n",
    "SMOTE helps to overcome the problem of imbalanced datasets by increasing the diversity of the minority class and reducing the bias towards the majority class during training. This can improve the performance of machine learning models by ensuring that the model is exposed to a more balanced representation of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1687238657613,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "Gx_INzq0n9Sv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming you have your pandas DataFrame df with features and target variable\n",
    "\n",
    "# Separate the features (X) and target variable (y) from the DataFrame\n",
    "X = data.drop('Defaulter', axis=1)\n",
    "y = data['Defaulter']\n",
    "\n",
    "# Initialize the SMOTE oversampling algorithm\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Convert X and y to NumPy arrays\n",
    "X_array = X.values\n",
    "y_array = y.values\n",
    "\n",
    "# Perform oversampling on the data\n",
    "X_resampled, y_resampled = smote.fit_resample(X_array, y_array)\n",
    "\n",
    "# Convert the resampled arrays back to a pandas DataFrame\n",
    "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "y_resampled_df = pd.DataFrame(y_resampled, columns=['target'])\n",
    "\n",
    "# Print the class distribution before and after oversampling\n",
    "print(\"Class distribution before oversampling:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(y_resampled_df['target'].value_counts())\n",
    "\n",
    "X= X_resampled_df\n",
    "y= y_resampled_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yk3W16S6bPXM"
   },
   "source": [
    "## **Split Data in training, validation, and testing datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1687238785716,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "k1bzW6qGruck"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#The test_size parameter is set to 0.2, indicating that 20% of the data will be allocated to the testing set, while the remaining 80% will be used for training.\n",
    "#The random_state parameter is set to 42 to ensure reproducibility of the split, meaning that the same random split will be obtained each time the code is executed.\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# val_x, test_x, val_y, test_y = train_test_split(test_x, test_xy, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cKpC89OKKQS"
   },
   "source": [
    "## **Model Training: Xgboost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPWT02DT4orW"
   },
   "source": [
    "### **Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1687238792591,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "NRQUDceR4rSF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "import neptune\n",
    "from neptune.integrations.xgboost import NeptuneCallback\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "un-libvu4wjY"
   },
   "source": [
    "### **Configure Neptune**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format:\n",
    "\n",
    "# Create a Neptune run object\n",
    "run = neptune.init_run(\n",
    "    project=\"your-workspace-name/your-project-name\",  \n",
    "    api_token=\"YourNeptuneApiToken\",  \n",
    "    tags=[\"quickstart\", \"script\"],  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4517,
     "status": "ok",
     "timestamp": 1687164465376,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "gr0HjGkoRCAD",
    "outputId": "4f072218-f43b-4cf0-a285-2637b2232d9f"
   },
   "outputs": [],
   "source": [
    "# Configure Neptune\n",
    "run = neptune.init_run(\n",
    "    project=\"portfolio/loan-default-prediction\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NDQ4NTg0NS02NjEzLTRmMzQtOWVmNy0yNDlkY2YzNzhhYTMifQ==\"\n",
    ")  # your credentials\n",
    "\n",
    "# Creating a NeptuneCallback object to integrate Neptune with XGBoost.\n",
    "neptune_callback = NeptuneCallback(run=run, log_tree=[0, 1, 2, 3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzc4_56T43Xz"
   },
   "source": [
    "### **Model training with hyperopt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jivXj44juThf"
   },
   "source": [
    "**XGBoost hyperparameters**\n",
    "***\n",
    "- **Boosting hyperparameters:** Control the gradient descent process in boosting.\n",
    "\n",
    "- **Tree hyperparameters:** Influence the construction of decision trees.\n",
    "\n",
    "- **Stochastic hyperparameters:** Determine the subsampling of training data during model building.\n",
    "\n",
    "- **Regularization hyperparameters:** Regulate model complexity to prevent overfitting.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145172,
     "status": "ok",
     "timestamp": 1687169062136,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "ZxStgynOFF0k",
    "outputId": "f0d5eb28-2892-411d-f6e4-24f9eef41fe4"
   },
   "outputs": [],
   "source": [
    "# Define search space for hyperparameter tuning of XGBoost model.\n",
    "search_space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -7, 0),\n",
    "    'max_depth': scope.int(hp.uniform('max_depth', 1, 100)),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -2, 3),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'gamma': hp.loguniform('gamma', -10, 10),\n",
    "    'alpha': hp.loguniform('alpha', -10, 10),\n",
    "    'lambda': hp.loguniform('lambda', -10, 10),\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'error',\n",
    "    'seed': 123,\n",
    "}\n",
    "train_x=train_x\n",
    "train_y=train_y\n",
    "test_x=test_x\n",
    "test_y=test_y\n",
    "\n",
    "\n",
    "# Finding the best hyperparameters using Hyperopt's fmin function.\n",
    "best_params = fmin(\n",
    "    fn=lambda params: train_model_xgboost(params, neptune_callback, train_x, train_y, test_x, test_y),\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=15,\n",
    "    rstate=np.random.default_rng(123)\n",
    ")\n",
    "run.stop()\n",
    "\n",
    "# Let's print the params\n",
    "print(best_params)\n",
    "\n",
    "# Rest of the code remains the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utxeVVRapjm5"
   },
   "source": [
    "### **Best fit xgboost model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "executionInfo": {
     "elapsed": 6910,
     "status": "ok",
     "timestamp": 1687169210802,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "goUrs5VNuC8L",
    "outputId": "a3ea623a-2425-4391-e1f3-fa969f7974e3"
   },
   "outputs": [],
   "source": [
    "# Access the best hyperparameters\n",
    "best_hyperparams = {k: best_params[k] for k in best_params}\n",
    "\n",
    "# Train the final XGBoost model with the best hyperparameters\n",
    "final_model = xgb.XGBClassifier(\n",
    "    max_depth=int(best_hyperparams['max_depth']),\n",
    "    learning_rate=best_hyperparams['learning_rate'],\n",
    "    gamma=best_hyperparams['gamma'],\n",
    "    subsample=best_hyperparams['subsample'],\n",
    "    colsample_bytree=best_hyperparams['colsample_bytree'],\n",
    "    random_state=42,\n",
    "    tree_method='hist',enable_categorical= True,  # Use GPU for faster training (if available)\n",
    ")\n",
    "\n",
    "final_model.fit(train_x, train_y)  # Train the final model on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ-8xNBvfyTw"
   },
   "source": [
    "### **Model validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1687169300696,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "s3Ii6crqJ91E",
    "outputId": "91acf697-db45-46b0-8061-ddccebd2436a"
   },
   "outputs": [],
   "source": [
    "# Assuming `test_x` contains your test feature data\n",
    "# Assuming `test_y` contains your test target labels\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = final_model.predict(test_x)\n",
    "\n",
    "# Print classification metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_y, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exJfYuvHf6RW"
   },
   "source": [
    "### **Save best fit model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_model, open('../Output/xgboost_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWvB3MHLReXw"
   },
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "filename = '../Output/xgboost_model.pkl'\n",
    "final_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "# Read the testing data to a file\n",
    "filename = '../Output/testing_data_iteration2.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    test_x, test_y = pickle.load(file)\n",
    "\n",
    "\n",
    "# Read the training data to a file\n",
    "filename2 = '../Output/training_data_iteration2.pkl'\n",
    "with open(filename2, 'rb') as file:\n",
    "    train_x, train_y= pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lp2gNsg6uVm3"
   },
   "source": [
    "## **Model Training: RandomForest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkjW6JOt0yJv"
   },
   "source": [
    "### **Random forest Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1354180,
     "status": "ok",
     "timestamp": 1687171571667,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "Eutd4JA7ubMc",
    "outputId": "fb4bfe49-3d6f-4266-80fd-30232d1c6da6"
   },
   "outputs": [],
   "source": [
    "# Define your parameter grid\n",
    "param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "best_parameters=random_forest_classifier_grid_search(param_grid, train_x, train_y)\n",
    "\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KviuTWbVXoBq"
   },
   "source": [
    "**Best Parameters:**\n",
    "- **'max_depth'**: None\n",
    "- **'min_samples_split'**: 2\n",
    "- **'n_estimators'**: 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqDwDJN90szh"
   },
   "source": [
    "### **Model Validation Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 179853,
     "status": "ok",
     "timestamp": 1687171804849,
     "user": {
      "displayName": "Arshman Tariq",
      "userId": "11298831622869106955"
     },
     "user_tz": -300
    },
    "id": "4WMXIuMDu3Yd",
    "outputId": "604b07eb-3328-4852-f65c-c9d1b840463d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Access the best hyperparameters\n",
    "best_hyperparams = {k: best_parameters[k] for k in best_parameters}\n",
    "\n",
    "# Train the randomforest model with the best hyperparameters\n",
    "final_model1 = RandomForestClassifier(\n",
    "    max_depth=best_hyperparams['max_depth'],\n",
    "    min_samples_split=best_hyperparams['min_samples_split'],\n",
    "    n_estimators=best_hyperparams['n_estimators'],\n",
    "     # Use GPU for faster training (if available)\n",
    ")\n",
    "\n",
    "final_model1.fit(train_x, train_y)  # Train the final model on the entire dataset\n",
    "\n",
    "\n",
    "# Assuming `test_x` contains your test feature data\n",
    "# Assuming `test_y` contains your test target labels\n",
    "\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = final_model1.predict(test_x)\n",
    "\n",
    "# Print classification metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_y, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bdm6IQAxBtW"
   },
   "source": [
    "### **Save Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6sf3ioNxHWF"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "filename = '../Output/RandomForest_model.pkl'\n",
    "\n",
    "# # save the model into the file\n",
    "pickle.dump(final_model1, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "# Load the model using the lines below\n",
    "# filename = 'drive/MyDrive/projectpro/1_explainable_ai/RandomForest_model.pkl'\n",
    "# final_model1 = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gsdV7bb9SMz_",
    "4e78b_idcMDT",
    "yk3W16S6bPXM",
    "Lp2gNsg6uVm3",
    "JkjW6JOt0yJv",
    "gqDwDJN90szh"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
